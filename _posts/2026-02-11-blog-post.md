---
title: 'Banning Kids or Redesigning Our Digital Future? The Ontological Politics of Platform Regulation'
date: 2026-02-11
permalink: /posts/2026/02/the-ontological-politics-of-platform-regulation/
tags:
  - social media
  - regulation
  - australia
  - annemarie mol
  - ontological politics
---

## Banning Kids or Redesigning Our Digital Future? The Ontological Politics of Platform Regulation

On 10 December 2025, Australia became the world’s first country to ban social media platforms for children under the age of 16. Several other countries, including Malaysia, Brazil, France, and Spain, are planning to take similar measures. The age ban clearly fails to address the root causes of the problems within these platforms–that they are designed to maximize profit. The result is what Nick Couldry refers to as “the space of the world,” designed by commercial entities, absorbing and commodifying the whole social space [^1]. If we are to redesign this space, we need to scrutinize these regulatory practices and disclose their ontological effects. For such an analysis, this paper proposes a turn towards practical ontology as a framework, which argues that we are not simply interacting with a pre-determined reality, but instead shaping *realities* through our practices. This is what Annemarie Mol refers to as *ontological politics*–realities enacted by a manifold of practices [^2]. 

If reality is done, Mol claims, then what we end up having is not one stable object, nor object as a set of separate entities, but instead, object as *multiples*–different versions of an entity that may clash, collaborate, and even depend on each other. Different ontologies of an object that exist within each other, enacted through various tools. To illustrate the concept, Mol gives two prominent examples of how anaemia is performed in its detection practices: *the clinical performance*, where the doctor listens to patient complaints and checks for visible symptoms; and *the laboratory performance*, where a norm is set based on the population data, and then the individual’s haemoglobin level is compared to the norm to see if they have anaemia. Importantly, these practices do not overlap–sometimes people may have no complaints or visible symptoms and yet have a haemoglobin level below the accepted threshold, and vice versa. Even though these methods diagnose something different, both are still valid practices of diagnosing anaemia [^3]. Thus, Mol concludes that anaemia is not a singular object that precedes its detection, instead, it has different realities that co-exist, and are enacted through different diagnostic performances. 

As performances are open processes that affect the very reality of objects, it is only natural that other interconnected objects will be affected by them as well. For instance, the statistical method for detecting anaemia not only enacts a particular type of anaemia, but also a particular type of sex difference. When setting statistical norms, practitioners partition the population into sub-groups and set different norms for each group to achieve better accuracy. These groups are children, men, women, and pregnant women. Thus, the statistical method performs sexes as a biologically separated dichotomy, and it also separates pregnant women from women. Even if these design choices make sense from a practical perspective–for example, the choice of separating pregnant women is due to the increase in blood volumes during pregnancy–they will enact the sexes such that humans can be separated into the categories of men and women based on their biological characteristics, and that pregnant women belongs to a distinct category that is neither men or women. Therefore, these practical decisions that seem to be technical refinements will also have ontological consequences in terms of sexual and biological differences.

Ontological politics of social media expose the constitutive effect of its regulatory practices, as different regulations will enact different ontologies of platforms, users, and online harm. Consequently, we need a critical examination of how age-banning practices perform these realities.

One of the most ubiquitous modern online surveillance practices involves inference systems, which appropriate and analyze vast user metadata to infer about user identities and behavioral habits. These practices include tracing and commodifying user activities in online spaces. Further, companies often utilize inference systems for conducting mass experiments on users through A/B testing to find optimal ways of manipulating online behavior, without users ever knowing they are being experimented upon. The implicit nature of these systems puts users into an ambiguous form of surveillance, where they are not sure which of their online activities are monitored by which agents, or whether they are being used as part of an experiment. On the other hand, the explicit identification systems operate as regulatory gatekeepers that decide who should be allowed in particular spaces, and who should be left out. These systems are integrated into gated platforms like online gambling sites or banking/finance applications, where users need to satisfy a set of predefined attributes in order to get in. As explicit identification requires sensitive information, these systems are often targeted by hacker attacks or leaks. While inference systems enact users as resources to be appropriated, experimented upon, and commodified; identification systems enact them as citizens to be governed and monitored.

In the Australian case [^4], platforms are using three age-assessment practices: The first is *the age inference method* that analyzes user data and online behavior to infer whether a user is old enough to have a social media account. This method operates implicitly, meaning that if the AI system can verify a user being over 16 by their data traces alone, then that user’s access will not be restricted. Since social media users can fill out their personal information without any verification, this method is rather susceptible to manipulation. In fact, there have been reports of underaged Australian social media users who bypassed the ban simply because their account age is listed as over 16. On the other hand, if the system cannot verify a user's age from their traces alone, or if it verifies a user to be younger than 16, then that user will not be allowed in. If a user believes that their access was restricted unconstitutionally, then they will need to resort to one of the other two methods. One is through a scan of an official document that includes their date of birth. This is *the age verification method*. The other is through a face scanning model that estimates their age. This is *the age estimation method*. While these methods are posed as alternatives to each other, they perform age differently: The first one performs age through extracting various forms of user data. This is composed of the user’s online behavior and their self-reported claims. The second one performs it through a government-issued document that contains their birthdate. In this case, the information on the document is granted absolute authority. The last one performs it through an AI estimation model. The user scans their face, and the model abstracts the facial features into pieces of data, processes this data, and then gives an estimated age. Thus, all three methods enact a different age that does not overlap–a user who was born 15 years ago may be estimated by the facial scanning model to be 18 years old, or the inference system may assess them to be 32 years old because that is what they have self-reported in their account’s personal information section. Further, these performances of age do not exist separately.  For instance, labeling training data, a process that is prerequisite for the age estimation method, utilizes the age verification method. How one of these methods is performed will affect the other two–they are highly dependent on each other. They *exist within* each other.


What is at stake when age is enacted in these different ways? User privacy is definitely a concern. Platforms outsource age verification systems to third-party companies, which often claim that they will not store users’ identification documents for more than a few days, and that they will not use the data for any other purpose. Still, people often prefer the age estimation method instead, because mass leaks of sensitive data have happened in the past [^5], and it has been hard to hold companies accountable in such occurrences. Further, public trust towards tech companies is diminished, and people do not believe in the corporate claims of safe use of sensitive data. Overall, facial scanning is seen as the safer option in terms of privacy, especially since users tend to share photos of their faces publicly on social media platforms anyway. On the other hand, the age estimation method enacts age in a particularly normative manner. Because the AI model assesses age with respect to its training dataset that consists of vast amounts of human faces, data collection and processing practices will affect the realities of age. For instance, the National Institute of Standards and Technology’s (NIST) report on age estimation technology [^6] shows that most of the age estimation algorithms are less accurate when assessing the age of women compared to men. Further, they have a particular difficulty when assessing the age of racial minorities. NIST claims that this difference in accuracy is largely caused by the underrepresentation of women and minorities in training datasets. As a result, bodies that do not conform to the normative values of the Western world see a greater divergence in their estimated age compared to the body of a white man. Consequently, people from marginalized communities who are older than 16 are more likely to be mislabeled as younger than 16. These people, who have been historically excluded from public spaces and over-policed, either have to fall back on the age identification method or lose their social media access altogether if they refuse to scan their ID.

The regulation of social media through the exclusion of children also affects the reality of social media harm. It performs social media as a neutral space by detaching the platform from its content, and by shifting the responsibility towards its users. When harm is treated as an inevitable outcome of public activity, the exclusion of those who are most susceptible to it may stand out as a viable solution. Hence, the regulation enacts children as an “at-risk” category, while rendering the economic incentives of social media companies invisible. On the contrary, a regulatory approach targeting the business model of social media platforms would enact social media harm as an economically motivated, deliberately engineered product of a data-driven advertising model. Such an approach would not practice harm as an inevitable byproduct, but as an outcome of a set of design choices made in accordance with the attention economy. It would enact social media companies as economically incentivized agents that are nonchalant towards fostering a healthy space of the world, and platform architectures as contested sites that consists of various design choices–such as the infinite scroll, notification bombarding, and gamified engagement [^7]–that perform users as, to quote from Jack Balkin, “pairs of eyeballs attached to a wallet” [^8]. 

If other countries follow Australia in implementing an age ban such that it becomes the norm, there will be several implications worth discussing. First and foremost, platforms will be performed as reified entities that are pre-given and objective, rather than contested sites that can be enacted differently. This, in my opinion, is the most fundamental effect of this regulation: that it concretizes platforms as the way they currently are. The algorithmic and economic architecture of platforms are no longer decisions but facts. This practice also enacts social media users as the isolated ground of social media harm–not only is the problem caused by the public, but the solution lies within them as well: finding a sufficient age threshold such that users below the threshold can be categorized as being susceptible to harm and thus denied entry. On the contrary, social media is deemed safe for those above the age threshold. Thus, the age ban performs social media harm as a problem of statistics, and users as citizens to be manipulated and filtered in order to minimize harm. If it is normalized, we may see it applied to other online spaces as well, paving the way for an online reality based on gated communities.

As social media platforms have become an infrastructure for the conditions of social interaction–the space of the world, as Couldry puts it–it is crucial to analyze how these platforms are practiced, including measurements taken to regulate them. This essay tried to utilize Annemarie Mol’s concept of ontological politics to propose a framework for analyzing the recently introduced social media ban in Australia. Notably, the age-based regulation renders platforms as neutral, concrete entities, and enacts harm as a byproduct of public engagement. As a result, it redistributes responsibility and constitutes what interventions are possible. It reorganizes a digital future where user governance is normalized, while platform power disappears from view. Users are performed as citizens who are classified and filtered based on a predetermined threshold, and age itself becomes a matter of probability that is unevenly distributed across bodies. This analysis makes it clear that the current debates concerning online will actively shape the space of the world of our digital future.


### Notes
[^1]: Find Nick Couldry’s recent book, The Space of The World, here https://www.politybooks.com/bookdetail?book_slug=the-space-of-the-world-can-human-solidarity-survive-social-media-and-what-if-it-cant--9781509554720
[^2]: See Mol, A. (1999). Ontological Politics. A Word and Some Questions. The Sociological Review, 47, 74-89. https://doi.org/10.1111/j.1467-954X.1999.tb03483.x 
[^3]: In fact, Mol goes on to explicate how these two ways of anaemia handling have historically interacted with and depended on each other.
[^4]: You can find a detailed report on Age Assessment Technologies from Australia’s Department of Infrastructure, Transport, Regional Development, Communications, Sport and the Arts here https://www.infrastructure.gov.au/department/media/publications/age-assurance-technology-trial-final-report
[^5]: For a recent example of a data leak caused by a third party age verification firm that Discord outsourced its age verification operations to, see https://www.bbc.com/news/articles/c8jmzd972leo
[^6]: You can access NIST’s report from here https://nvlpubs.nist.gov/nistpubs/ir/2024/NIST.IR.8525.pdf
[^7]: To learn more about the taxonomy of harmful platform design choices, see https://docs.google.com/spreadsheets/d/1GVO7sNuCNmNwqVK64PHQI7wxd8-Gmr9PqdkW12elmus/edit?gid=941162555#gid=941162555
[^8]: For a well-articulated argument by Jack Balkin on regulating the business model of social media companies, see https://knightcolumbia.org/content/how-to-regulate-and-not-regulate-social-media
